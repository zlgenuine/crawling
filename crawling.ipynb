{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658c51c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktinische Keratose und Plattenepithelkarzinom der Haut. S3-LL (Leitlinienprogramm Onkologie der AWMF, DKG und DKH)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from Bio import Entrez\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "titles_pth = \"concensus.csv\"\n",
    "df = pd.read_csv(titles_pth)\n",
    "# print(df.shape)\n",
    "# print(df.columns)\n",
    "for title in df[\"Title\"][173:174]:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from Bio import Entrez\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "base_url = \"https://zjuapi.com/v1\"\n",
    "api_key = \"sk-6psVESKMGkzC4lPHHI46FIdkacdT7rWOpijx5rFhXlaZzydw\"\n",
    "client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "titles_pth = \"concensus.csv\"\n",
    "df = pd.read_csv(titles_pth)\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "for title in df[\"Title\"][0:5]:\n",
    "    print(\"title:\"+title)\n",
    "    client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "    prompt = f\"\"\"\n",
    "    You are a link selector. \n",
    "    Given a list of PDF URLs for the query \"{title}\", choose the single most official and reliable PDF link. \n",
    "    Return only the URL, no text, no explanation, no markdown.\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        tools=[\n",
    "            {\"type\":\"web_search\"},\n",
    "        ],\n",
    "        input=prompt\n",
    "    )\n",
    "\n",
    "    response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=prompt)\n",
    "\n",
    "print(\"link:\"+response.output_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269bdea6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb11f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def search_pdf_link(query, serpapi_key, num=10):\n",
    "    \"\"\"\n",
    "    1) 用关键词搜索\n",
    "    2) 筛选 .pdf 链接\n",
    "    3) 返回第一个 pdf 链接\n",
    "    \"\"\"\n",
    "    url = \"https://serpapi.com/search.json\"\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"num\": num,\n",
    "        \"api_key\": serpapi_key\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, params=params)\n",
    "    data = res.json()\n",
    "\n",
    "    # 过滤出 PDF 链接\n",
    "    pdf_links = []\n",
    "    for item in data.get(\"organic_results\", []):\n",
    "        link = item.get(\"link\", \"\")\n",
    "        if link.lower().endswith(\".pdf\"):\n",
    "            pdf_links.append(link)\n",
    "\n",
    "    return pdf_links\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 你的 SerpAPI Key\n",
    "    serpapi_key = \"e3e5bdf3db175a00f6fdd33fabaacd1032b9680f7c07e1e4db25540f13f7628e\"\n",
    "\n",
    "    # 论文标题/关键词\n",
    "    query = \"A European Academy of Neurology guideline on medical management issues in dementia pdf\"\n",
    "\n",
    "    pdf_links = search_pdf_link(query, serpapi_key)\n",
    "    print(pdf_links[0] if pdf_links else \"No PDF found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2baba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== START ====\n",
      "titles count: 5\n",
      "['A European Academy of Neurology guideline on medical management issues in dementia', 'Allergie présumée aux pénicillines – Évaluation des risques pour un usage optimal et sécuritaire des bêta-lactamines', 'Détection de l’entérocoque résistant à la vancomycine (ERV)', 'Diagnostik und Therapie der Vitiligo. S2k-LL (DDG)', 'Diagnostik und Therapie von Gedächtnisstörungen bei neurologischen Erkrankungen. S2e-LL (DGN)']\n",
      "A European Academy of Neurology guideline on medical management issues in dementia\n",
      "A European Academy of Neurology guideline on medical management issues in dementia\n",
      "Allergie présumée aux pénicillines – Évaluation des risques pour un usage optimal et sécuritaire des bêta-lactamines\n",
      "Allergie présumée aux pénicillines – Évaluation des risques pour un usage optimal et sécuritaire des bêta-lactamines\n",
      "Détection de l’entérocoque résistant à la vancomycine (ERV)\n",
      "Détection de l’entérocoque résistant à la vancomycine (ERV)\n",
      "Diagnostik und Therapie der Vitiligo. S2k-LL (DDG)\n",
      "Diagnostik und Therapie der Vitiligo. S2k-LL (DDG)\n",
      "Diagnostik und Therapie von Gedächtnisstörungen bei neurologischen Erkrankungen. S2e-LL (DGN)\n",
      "==== END ====\n",
      "Diagnostik und Therapie von Gedächtnisstörungen bei neurologischen Erkrankungen. S2e-LL (DGN)\n",
      "==== END ====\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from Bio import Entrez\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def search_pdf(title):\n",
    "    url = \"http://104.194.90.24:18081/tavilysearch\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer sk-7izCWzKkQXDWuxU9FYP85mIg8SNSfUOn93MBEQtqdTr37K7T\"\n",
    "    }\n",
    "    data = {\"query\": title + \" pdf\"}\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    results = response.json().get(\"results\", [])\n",
    "    #print(f\"status_code: {response.status_code}\")\n",
    "    return [r.get(\"url\",\"\") for r in results if r.get(\"url\",\"\").lower().endswith(\".pdf\")]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    titles_pth = \"concensus.csv\"\n",
    "    df = pd.read_csv(titles_pth)\n",
    "    titles = df[\"Title\"].iloc[:5].tolist()\n",
    "    for title in titles:\n",
    "        pdfs = search_pdf(title)\n",
    "        print(title)\n",
    "        # if pdfs:\n",
    "        #     print(f\"pdfs: {pdfs}\")\n",
    "        # else:\n",
    "        #     print(f\"no pdf found. top result: {pdfs[0]['url'] if pdfs else 'none'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
